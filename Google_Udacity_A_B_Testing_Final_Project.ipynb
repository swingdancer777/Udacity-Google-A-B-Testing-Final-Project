{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIPndut0bEvQ"
   },
   "source": [
    "#**Google-Udacity A/B Testing-Final Project**\n",
    "\n",
    "# Resources used for this project\n",
    "1. The final project: https://classroom.udacity.com/courses/ud257/lessons/4126079196/concepts/42072285530923\n",
    "2. The directions: https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True\n",
    "3. The template: https://docs.google.com/document/d/16OX2KDSHI9mSCriyGIATpRGscIW2JmByMd0ITqKYvNg/edit\n",
    "4. The final results: https://docs.google.com/spreadsheets/d/1Mu5u9GrybDdska-ljPXyBjTpdZIUev_6i7t4LRDfXM8/edit#gid=0\n",
    "5. The online calculator for \"Sample size\": http://www.evanmiller.org/ab-testing/sample-size.html\n",
    "6. The online calculator for \"Sign and binomial test\": http://graphpad.com/quickcalcs/binomial1.cfm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oN3iCfj9j7dR"
   },
   "source": [
    "## A. Experiment Overview: Free Trial Screener\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. This screenshot below shows what the experiment looks like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAFZ3Je3UZK9"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1ee6whl-U_dBEV3b7wWHsvk6U8icIV-1v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DPpt2P-kl4B"
   },
   "source": [
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVPkFjf2ksmx"
   },
   "source": [
    "## B. Metric Choice\n",
    "Which of the following metrics would you choose to measure for this experiment and why? For each metric you choose, indicate whether you would use it as an invariant metric or an evaluation metric. The practical significance boundary for each metric, that is, the difference that would have to be observed before that was a meaningful change for the business, is given in parentheses. All practical significance boundaries are given as absolute changes.\n",
    "\n",
    "Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) User- ids are automatically unique since the site does not allow the same user-id to enroll twice.\n",
    "1.\t**Number of cookies:** That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "2.\t**Number of user-ids:** That is, number of users who enroll in the free trial. (dmin=50)\n",
    "3.\t**Number of clicks:** That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "4.  **Click-through-probability:** That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "5.\t**Gross conversion:** That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "6.\t**Retention:** That is, number of user-ids to remain enrolled past the 14- day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "7.\t**Net conversion:** That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)\n",
    "\n",
    "You should also decide now what results you will be looking for in order to launch the experiment. Would a change in any one of your evaluation metrics be sufficient? Would you want to see multiple metrics all move or not move at the same time in order to launch? This decision will inform your choices while designing the experiment.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOBzGt706EXY"
   },
   "source": [
    "## **Metric Choice**\n",
    "**Invariant Metrics:** number of cookies, number of clicks, click-through-probability\n",
    "\n",
    "**Evaluation Metrics:** gross conversion, retention, net conversion\n",
    "\n",
    "## **Invariant Metrics**\n",
    "\n",
    "These are metrics which are not expected to change or vary much between the control and test groups. These metrics are used to do a \"sanity check\" to make sure that the control and test groups are comparable and the experiment was set up properly.\n",
    "\n",
    "**1. Number of cookies:** This is the number of unique cookies to view the course overview page. This is the unit of diversion. This an invariant metric because we would expect (and desire) the number of cookies sent to the control and test groups to be very similar so that our results would be valid.\n",
    "\n",
    "**2. Number of Clicks:** This is the number of unique cookies to click the \"Start free trial\" button (which happens before the \"free trial\" screener is triggered). This is an invariant metric. At this point in the funnel, the user has not seen the \"free trial screener\" page, so a similar number of users should click the \"Start free trial\" button in the control and test groups.\n",
    "\n",
    "**3. Click-through-probability:** This is the number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. This is an invariant metric. Again, at this point in the funnel, the user has not seen the \"free trial screener\" page, so the click-through-probability should be similar in the control and test groups.\n",
    "\n",
    "## **Evaluation Metrics**\n",
    "\n",
    "These metrics are chosen because there may be a different distribution between the control and test groups as a result of the experiment. These metrics have a minimum level of statistical significance and practical significance that must be met/observed in order to launch the change.\n",
    "\n",
    "The goals are to 1) reduce the number of frustrated students who quit the free trial 2) without significantly reducing the number of students who continue past the free trial, pay, and finish the course, and 3) use the limited coaching resources on students who are more likely to finish the course.\n",
    "\n",
    "As a result, the following outcomes are desired in the test group:\n",
    "1. Decreased gross conversion. This would mean that students who cannot commit the needed amount of time are dissuaded from enrolling in the free trial.\n",
    "2. Increased retention. This would mean that the ratio of users enrolled past the 14-day free trial compared to users who completed checkout would increase. In other words, more users who signed up for the free trial stayed past the 14-day free trial period.\n",
    "3. Increased net conversion. This would mean that more users who signed up for the free trial stayed beyond the 14-day free trial period. (This is similar to retention, but measured differently.)\n",
    "\n",
    "**1. Gross conversion:** This is the number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. This is an evaluation metric because, with the \"free trial screener\" page, we expect this metric to be lower for the test group vs the control group.\n",
    "\n",
    "**2. Retention:** This is the number of user-ids to remain enrolled past the 14-day free trial (and thus make at least one payment) divided by the number of user-ids to complete checkout. This is an evaluation metric. If a student enrolls in the 14-day free trial, then we track their user-ID going forward.  With the test group receiving the \"free trial screener\" page, we hope retention would be higher vs the control group because students who were not sufficiently committed did not sign up.\n",
    "\n",
    "**3. Net conversion:** This is the number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. In the test group, we do not want the screener to *significantly reduce* the number of students who continue past the free trial and ultimately complete the course. This would mean that the net conversion should remain the same (within a confidence interval). Though unexpected, net conversion might increase. If so, this would suggest that more users who signed up for the free trial even after seeing the \"free-trial-screener-page\" were committed and completed the course.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYBmrSetoCAm"
   },
   "source": [
    "## C. Measuring Variability\n",
    "Udacity provides the baseline values for these metrics. (Note: these numbers have been changed from Udacity's true numbers.)\n",
    "\n",
    "* Unique cookies to view course overview page per day: 40000\n",
    "* Unique cookies to click \"Start free trial\" per day: 3200\n",
    "* Enrollments per day: 660\n",
    "* Click-through-probability on \"Start free trial\": 0.08\n",
    "* Probability of enrolling, given click: 0.20625\n",
    "* Probability of payment, given enroll: 0.53\n",
    "* Probability of payment, given click 0.1093125\n",
    "\n",
    "For each metric you selected as an evaluation metric, estimate its standard deviation analytically. Do you expect the analytic estimates to be accurate? That is, for which metrics, if any, would you want to collect an empirical estimate of the variability if you had time?\n",
    "\n",
    "---\n",
    "\n",
    "**Estimating Standard Deviation**\n",
    "\n",
    "Let's calculate analytically the Standard Deviation for our Evaluation Metrics using the baseline values. This helps us set our expectations BEFORE the experiment.\n",
    "\n",
    "Gross conversion and net conversion are both probabilities, so we can assume they follow a binomial distribution. And the binomial distribution will take on a normal distribution for a large enough sample size (which we have). For a normal distribution, the estimated standard deviation is sqrt([p-hat * (1-p-hat)]/N).\n",
    "\n",
    "We are asked to make an analytic estimate of the standard deviation of the evaluation metrics given a sample size of 5000 cookies visiting the course overview page.\n",
    "\n",
    "* Number of cookies = 5000\n",
    "* Number of clicks on \"Start free trial\" = 5000 cookies x 0.08 clicks/cookie = 400 clicks\n",
    "* Number of enrollments = 5000 cookies x 0.08 clicks/cookie x 0.20625 enrollments/click = 82.5 enrollments\n",
    "\n",
    "**Gross conversion** = Use the given baseline \"Probability of enrolling, given click\" = 0.20625\n",
    "\n",
    "$SD-Gross-conversion = \\sqrt{\\frac{p*(1-p)}{n}} = \\sqrt{\\frac{0.20625*(1-0.20625)}{400}} = 0.0202$\n",
    "\n",
    "**Net conversion** = Use the given baseline \"Probability of payment, given click\" = 0.1093125\n",
    "\n",
    "$SD-Net-conversion = \\sqrt{\\frac{p*(1-p)}{n}}  = \\sqrt{\\frac{0.1093125*(1-0.1093125)}{400}} = 0.0156$\n",
    "\n",
    "**Retention** = Use the given baseline \"Probability of payment, given enroll\" = 0.53\n",
    "\n",
    "$SD-Retention = \\sqrt{\\frac{p*(1-p)}{n}} = \\sqrt{\\frac{0.53*(1-0.53)}{400}} = 0.0549$\n",
    "\n",
    "**For gross conversion**, I would expect the analytic variance estimate to be accurate or close to its empirical variance. Gross conversion is # userIDs enrolled / # of cookies that started the free trial. In other words, the probability of enrollment, given a click. In this case, the unit of diversion (the numerator) is cookies, as this is the unit by which the samples are originally assigned to test and control groups. And the unit of analysis (the denominator) is also cookies (cookies that click). When both numerator and denominator are the same unit (cookies), the analytic variance will be very close to the empirical variance.\n",
    "\n",
    "**For net conversion**, I would expect also the analytic variance estimate to be accurate or close to its empirical variance. Net conversion is # userIDs who pay / # of cookies that started the free trial. In other words, the probability of payment, given a click. In this case, the unit of diversion (the numerator) is cookies, as this is the unit by which the samples are originally assigned to test and control groups. And the unit of analysis (the denominator) is also cookies (cookies that pay). When both numerator and denominator are the same unit (cookies), the analytic variance will be very close to the empirical variance.\n",
    "\n",
    "**For retention**, I would expect the analytic variance to be different from the empirical variance. Retention is # userIDs who pay / # of cookies that started the free trial. In other words, the probability of payment, given a click. In this case, the unit of diversion (the numerator) is cookies, as this is the unit by which the samples are originally assigned to test and control groups. But the unit of analysis (the denominator) is userIDs who complete checkout and enroll. Since both numerator and denominator are NOT the same unit (cookies vs userIDs), the analytic variance will NOT be very close to the empirical variance. We would want to estimate the variance empirically also if we have the data to do so.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mGs4V8Zoysg"
   },
   "source": [
    "## D. Sizing\n",
    "\n",
    "1. **Choosing Number of Samples given Power**\n",
    "\n",
    "Using the analytic estimates of variance, how many pageviews total (across both groups) would you need to collect to adequately power the experiment? Use an alpha of 0.05 and a beta of 0.2. Make sure you have enough power for\n",
    "each metric.\n",
    "\n",
    "I used the online calculator (#5 in \"Resources used for this project\") to calculate the sample sizes for the Evaluation Metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWnTMk0GnVCw"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## **Pageviews needed for Gross Conversion**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Pvs93-_aSY99OOUdYT9M0ZEhFBCDLrfJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ezgyg2E2viqF"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **UserIDs needed for Retention**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1uhqi1f7cGHut13CWKDBZuqwGjiLPnCln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upGBrpj_yZG0"
   },
   "source": [
    "---\n",
    "## **Pageviews needed for Net Conversion**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1LOuITyxa49Lnmm8A8WGuHkky5G-tj8MB)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_1fL5R5baI9"
   },
   "source": [
    "From these calculations, we can determine the minimum number of pageviews needed to conduct the experiment. The metric with the largest number of pageviews will be the minimum number of pageviews needed.\n",
    "\n",
    "Each number of pageviews calculated above is for one group of the experiment. So we need to double the number for both test and control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYhBJXy0e7Lx"
   },
   "source": [
    "**Gross conversion**\n",
    "\n",
    "Gross conversion = # of user IDs who enroll / # of cookies who start the free trial. The unit of analysis is cookies.\n",
    "\n",
    "Based on the online calculator, we need 25,835 cookies/group x 2 groups = 51,670 cookies to start the free trial.\n",
    "\n",
    "Udacity tells us that the click-through-probability of cookies that see the course overview and then click \"Start free trial\" is 8%.\n",
    "\n",
    "51,760 cookies / .08 = 645,875 cookies\n",
    "\n",
    "So if we have 645,875 cookies see the course overview page, 8% will start the free trial. This will generate the 51,670 cookies needed to run the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5WXyRMEgTxl"
   },
   "source": [
    "**Retention**\n",
    "\n",
    "Retention = # userIDs who pay / # userIDs who enroll.  The unit of analysis is userIDs who enroll.\n",
    "\n",
    "Based on the online calculator, we need 39,115 userIDs/group x 2 groups = 78,230 userIDs to enroll.\n",
    "\n",
    "The rate of userIDs who enroll / number of cookies (AKA pageviews of the course overview) = 660 enrollments / 40,000 pageviews = .0165 = 1.65%.\n",
    "\n",
    "78,230 userIDs / .0165 = 4,741,212.12 pageviews\n",
    "\n",
    "If we have 4,741,212.12 pageviews of the course overview page, and 1.65% of those userIDs will pay, then you will get the 78,230 userIDs needed for the experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reAsGVyri9xC"
   },
   "source": [
    "**Net conversion**\n",
    "\n",
    "Net conversion = # userIDs who pay / # unique cookies who enroll.  The unit of analysis was cookies.\n",
    "\n",
    "Based on the online calculator, we need 27,413 cookies/group x 2 groups = 54,826 cookies to start the free trial.\n",
    "\n",
    "Udacity tells us that the click-through-probability of cookies that see the course overview and then click \"Start free trial\" is 8%.\n",
    "\n",
    "54,826 cookies / .08 = 685,325 cookies\n",
    "\n",
    "So if we have 685,325 cookies see the course overview page, 8% will start the free trial. This will generate the 78,320 paying userIDs needed to run the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCwlRAB_AJn7"
   },
   "source": [
    "**Bonferroni correction**\n",
    "\n",
    "When testing more than one metric, often the significance level of each test is adjusted to achieve a desired overall significance level for all three tests. One way to do this is to use the Bonferroni correction. However, I did not use the Bonferroni correction because these three metrics are likely not independent of each other. (That is, when one of these metrics changes, the other two metrics are likely to change also because these metrics are related.) The Bonferroni correction would be too conservative and would cause the required sample size to be larger than truly needed. It would increase the chance that we might miss a significant effect that was truly there (ie, a type II error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhbR4Xx6V94A"
   },
   "source": [
    "2. **Choosing Duration vs. Exposure**\n",
    "\n",
    "What percentage of Udacity's traffic would you divert to this experiment (assuming there were no other experiments you wanted to run simultaneously)? Is the change risky enough that you wouldn't want to run on all traffic?\n",
    "\n",
    "Given the percentage you chose, how long would the experiment take to run, using the analytic estimates of variance? If the answer is longer than a few weeks, then this is unreasonably long, and you should reconsider an earlier decision.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9X49-oYFR_T"
   },
   "source": [
    "**Duration Using Retention**\n",
    "\n",
    "Based on the retention calculation, we need 4,741,212 pageviews to run the test. Udacity's daily traffic is 40,000 pageviews per day.\n",
    "4,741,212 pageviews / 40,000 pageviews per day = 118.53 days\n",
    "\n",
    "119 days is too long to run this test!  If we sent all the traffic on Udacity to this test (at a rate of 40,000 pageviews per day), it would take nearly 4 months to complete the experiment! This is too long, and we don't want to send 100% of the traffic to the experiment in case unforeseen negative side effects appear. We would not want that to affect all of the traffic.\n",
    "\n",
    "Negative side effects could include a larger than expected decrease in enrollment and the opportunity cost of not being able to run other experiments (if 100% of traffic is sent to this test). \n",
    "\n",
    "So we need to reduce the number of pageviews that are required to run this test.\n",
    "\n",
    "The limiting metric is retention. It requires the most pageviews to do a test. For this metric, we could reduce the statistical power of the test (AKA \"sensitivity\") (power = 1 â€“ beta) or raise the significance level, alpha, of the test. Both would reduce the number of cookies needed. We could also raise the minimum practical significance required from 1% to 2%. Doing this would only require 9780 x 2 = 19,560 userIDs who enroll.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kjemyv_iRWvx"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1XajncKSLjMxTVPvmUOGOB3o7Ul3S8gDp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMqA4BU9S1bc"
   },
   "source": [
    "19,560 userIDs who enroll / .0165% userIDs who enroll per pageview = 1,185,454.55 pageviews. Round this to 1,185,455 pageviews needed.\n",
    "\n",
    "1,185,455 pageviews / 40,000 pageviews per day = 29.6 days needed using 100% traffic\n",
    "\n",
    "1,185,455 pageviews / 20,000 pageviews per day = 59.3 days needed using 50% traffic\n",
    "\n",
    "Completing the experiment in about 30 days would be reasonable, but we would still need to use 100% of the website's traffic to do it, which is not preferred. More importantly, the traffic data available from Udacity does not provide the data covering the entire period to analyze retention.  So we won't consider retention but instead will continue with gross and net conversion metric analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgczhI7WTcYz"
   },
   "source": [
    "**Duration Using Gross and Net Conversion**\n",
    "\n",
    "If we just measure gross and net conversion metrics, the previous calculations show we need 645,875 pageviews for gross conversion and 685,325 pageviews for net conversion. Net conversions require a larger number of pageviews, so we will analyze that.\n",
    "\n",
    "685,325 pageviews / 40,000 pageviews per day = 17.13 days, which we round up to 18 days. This I just about 2 weeks and 4 days and would be an acceptable length of time to run an experiment. But we still prefer not to send 100% of the website traffic through this experiment. \n",
    "\n",
    "So if we use just 50% of the traffic, 685,325 / 20,000 pageviews per day = 34.26 days, which we round up to 35 days. This is still an acceptable length of time to run the experiment, and we don't have to send all the traffic to the experiment. \n",
    "\n",
    "If we want to get the experiment done within one month, we can send 60% of the traffic to the experiment. 40,000 pageviews per day x 60% = 24,000 page views per day. 685,325 pageviews / 24,000 pageviews per day = 28.55 days, which rounds up to 29 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fh8P7Uj4pAII"
   },
   "source": [
    "## E. Analysis\n",
    "The daily experiment data to analyze is [in this Google Spreadsheet](https://docs.google.com/spreadsheets/d/1p9PjyevoikQyqL2eq8SLpRGEzeJWIIaBcQvz8cCbatA/edit#gid=971768347). This data contains the raw information needed to compute the above metrics, broken down day by day. Note that there are two sheets within the spreadsheet - one for the experiment group, and one for the control group.\n",
    "\n",
    "The meaning of each column is:\n",
    "1. **Pageviews:** Number of unique cookies to view the course overview page that day.\n",
    "2. **Clicks:** Number of unique cookies to click the course overview page that day.\n",
    "3. **Enrollments:** Number of user-ids to enroll in the free trial that day.\n",
    "4. **Payments:** Number of user-ids who who enrolled on that day to remain enrolled for 14 days and thus make a payment. (Note that the date for this column is the start date, that is, the date of enrollment, rather than the date of the payment. The payment happened 14 days later. Because of this, the enrollments and payments are tracked for 14 fewer days than the other columns.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4h5ttLAFydwt"
   },
   "source": [
    "## E1. Sanity Checks\n",
    "Start by checking whether your invariant metrics are equivalent between the two groups. If the invariant metric is a simple count that should be randomly split between the 2 groups, you can use a binomial test as demonstrated in Lesson 5. Otherwise, you will need to construct a confidence interval for a difference in proportions using a similar strategy as in Lesson 1, then check whether the difference between group values falls within that confidence level.\n",
    "\n",
    "If your sanity checks fail, look at the day by day data and see if you can offer any insight into what is causing the problem.\n",
    "\n",
    "---\n",
    "**Sanity Check for Number of Cookies**\n",
    "\n",
    "Find the 95% confidence interval for the average number of cookies that should be in the control and test groups.\n",
    "\n",
    "Prob(cookie is in control group) = 0.5\n",
    "\n",
    "Control group = 345,543 cookies\n",
    "\n",
    "Test group = 344,660 cookies\n",
    "\n",
    "Total of both groups = 690,203\n",
    "\n",
    "The event can be defined as \"success\" = in control group and \"failure\" = in test group.\n",
    "\n",
    "This is a binomial event, so it follows a binomial distribution. If the sample size is large enough, we can assume the distribution is normal. The test is: if sample size $(N) * \\hat{p} > 5$, you can assume the distribution is normal.\n",
    "\n",
    "$\\hat{P} (in\\_control\\_group)$ = 345,543/690,203 = 0.5006\n",
    "\n",
    "$N * \\hat{P}$ = 690,203 * 0.5006 = 345,543. This is greater than 5, so we can assume the distribution is normal.\n",
    "\n",
    "Confidence Interval = point estimate +/- margin of error\n",
    "\n",
    "Confidence Interval = point estimate +/- z-score-for-95%-interval * standard error\n",
    "\n",
    "$Confidence\\: Interval = 0.5 +/- 1.96 * \\sqrt{\\frac{p*(1-p)}{n}}$\n",
    "\n",
    "$Confidence\\: Interval = 0.5 +/- 1.96 * \\sqrt{\\frac{0.5*(1-0.5)}{690,203}}$\n",
    "\n",
    "Confidence Interval = 0.5 +/- 1.96 * 0.0006018\n",
    "\n",
    "Confidence Interval = 0.5 +/- 0.00118\n",
    "\n",
    "So the confidence interval is (0.4988 to 0.5012).\n",
    "\n",
    "The proportion we observed is 0.5006, which IS in the confidence interval. This means the difference in the number of cookies in the test and control groups is expected. It is due to chance and not due to a real difference in the two groups. The number of cookies passes the sanity check!\n",
    "\n",
    "---\n",
    "**Sanity Check for Number of Clicks**\n",
    "\n",
    "Find the 95% confidence interval for the number of clicks on \"Start free trial\" that should be in the control and test groups.\n",
    "\n",
    "Prob(click is in control group) = 0.5\n",
    "\n",
    "Control group = 28,378 clicks\n",
    "\n",
    "Test group = 28,235 clicks\n",
    "\n",
    "Total of both groups = 56,703\n",
    "\n",
    "The event can be defined as \"success\" = (click is in control group) and \"failure\" = (click is in test group).\n",
    "\n",
    "This is a binomial event, so it follows a binomial distribution. If the sample size is large enough, we can assume the distribution is normal. The test is: if sample size $(N) * \\hat{p} > 5$, you can assume the distribution is normal.\n",
    "\n",
    "$\\hat{P} (in\\_control\\_group)$ = 28,378/56,703 = 0.5005\n",
    "\n",
    "$N * \\hat{P}$ = 56,703 * 0.5005 = 28,378. This is greater than 5, so we can assume the distribution is normal.\n",
    "\n",
    "Confidence Interval = point estimate +/- margin of error\n",
    "\n",
    "Confidence Interval = point estimate +/- z-score-for-95%-interval * standard error\n",
    "\n",
    "$Confidence\\: Interval = 0.5 +/- 1.96 * \\sqrt{\\frac{p*(1-p)}{n}}$\n",
    "\n",
    "$Confidence\\: Interval = 0.5 +/- 1.96 * \\sqrt{\\frac{0.5*(1-0.5)}{56,703}}$\n",
    "\n",
    "Confidence Interval = 0.5 +/- 1.96 * 0.00209\n",
    "\n",
    "Confidence Interval = 0.5 +/- 0.0041\n",
    "\n",
    "So the confidence interval is (0.4959 to 0.5041).\n",
    "\n",
    "\n",
    "\n",
    "The proportion we observed is 0.5005, which IS in the confidence interval. This means the difference in the number of cookies in the test and control groups is expected. It is due to chance and not due to a real difference in the two groups. The number of clicks passes the sanity check!\n",
    "\n",
    "---\n",
    "**Sanity Check for Click-Through Probability**\n",
    "\n",
    "Here we want to make sure the proportion of clicks given a pageview (our observed click-through-probability or CTP) is similar in the test and control groups. (This is because this is an invariant metric and should not change due to the experiment.) For this check, we will calculate the CTP for each group and then find the 95% confidence interval for the expected difference between the two click-through-probabilities.\n",
    "\n",
    "Null hypothesis: the test and control groups have the same probability of completing a checkout\n",
    "\n",
    "Alternate hypothesis: the test and control groups do not have the same probability of completing a checkout \n",
    "\n",
    "$H_0:$ CTP(test group) - CTP(control group) = 0; in other words d = 0\n",
    "\n",
    "$H_A:$ CTP(test group) - CTP(control group) $\\neq$ 0; in other words d $\\neq$ 0\n",
    "\n",
    "CTP(test group) = 28,325 / 344,660 = 0.08218\n",
    "\n",
    "CTP(control group) = 28,378 / 345,543 = 0.08213\n",
    "\n",
    "CTP(test group) - CTP(control group) = $\\hat{d}$ = 0.00005, which rounds to 0.0001\n",
    "\n",
    "Because we expect the test and control groups to be the same, we also expect that the variance of these two groups is the same. So we can use the pooled standard error in our confidence interval.\n",
    "\n",
    "$SD-pooled = \\sqrt{\\hat{P}pool(1-\\hat{P}pool)(\\frac{1}{Ncontrol}+\\frac{1}{Ntest})}$\n",
    "\n",
    "with \n",
    "\n",
    "$\\hat{P}pool = \\frac{Xcontrol + Xtest}{Ncontrol+Ntest}$\n",
    "\n",
    "\n",
    "$\\hat{P}pool = \\frac{28,378+28,325}{345,543+344,660} = 0.08215$\n",
    "\n",
    "$SD-pooled = \\sqrt{0.08215(1-0.08215)(\\frac{1}{345,543}+\\frac{1}{344,660})} = 0.00066105$\n",
    "\n",
    "Confidence Interval = point estimate +/- margin of error\n",
    "\n",
    "Confidence Interval = point estimate +/- z-score-for-95%-interval * standard error-pooled\n",
    "\n",
    "Confidence Interval = 0.5 +/- 1.96 * 0.00066105\n",
    "\n",
    "Confidence Interval = 0.5 +/- 0.0013\n",
    "\n",
    "So the confidence interval is (-0.0013 to 0.0013).\n",
    "\n",
    "$\\hat{d}$ = 0.0001. $\\hat{d}$ IS in the confidence interval. So the difference in click-through-probability is only due to chance, and there is no stastically significant difference between the two click-through probabilities. Click-through-probability passes the sanity check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N5jmIG9YyY1l"
   },
   "source": [
    "## E2. Check for Practical and Statistical Significance\n",
    "Next, for your evaluation metrics, calculate a confidence interval for the difference between the experiment and control groups, and check whether each metric is statistically and/or practically significant.\n",
    "\n",
    "**Note 1:** A metric is statistically significant if the confidence interval does not include 0 (that is, you can be confident there was a change), and it is practically significant if the confidence interval does not include the practical significance boundary (that is, you can be confident there is a change that matters to the business.)\n",
    "\n",
    "**Note 2:** The data given provided pageviews and clicks for 39 days, but only provided enrollments and payments for 23 days. So in calculations with enrollments and payments, we should only use the same 23 days of pageviews and clicks (not all 39 days).\n",
    "\n",
    "If you have chosen multiple evaluation metrics, you will need to decide whether to use the Bonferroni correction. When deciding, keep in mind the results you are looking for in order to launch the experiment. Will the fact that you have multiple metrics make those results more likely to occur by chance than the alpha level of 0.05?\n",
    "\n",
    "---\n",
    "**Gross Conversion**\n",
    "\n",
    "Control Group:\n",
    "Clicks = 17,293. Enrollments = 3785. Payments = 2033\n",
    "\n",
    "Test Group:\n",
    "Clicks = 17,260. Enrollments = 3423. Payments = 1945\n",
    "\n",
    "$\\hat{P}pooled = \\frac{Xcontrol + Xtest}{Ncontrol+Ntest}$\n",
    "\n",
    "$SD-pooled = \\sqrt{\\hat{P}pool(1-\\hat{P}pool)(\\frac{1}{Ncontrol}+\\frac{1}{Ntest})}$\n",
    "\n",
    "$\\hat{P}pooled\\_to\\_enroll = \\frac{3785+3423}{17,293+17,260} = 0.208607$\n",
    "\n",
    "$SD-pooled\\_to\\_enroll = \\sqrt{0.2086(1-0.2086)(\\frac{1}{17,293}+\\frac{1}{17,260})} = 0.004372$\n",
    "\n",
    "$\\hat{P}control = \\frac{3785}{17,293} = 0.21887$\n",
    "\n",
    "$\\hat{P}test = \\frac{3423}{17,260} = 0.19832$\n",
    "\n",
    "$H_0: P(test\\_group) - P(control\\_group) = 0; d = 0$\n",
    "\n",
    "$H_A: P(test\\_group) - P(control\\_group) = 0; d\\neq 0$\n",
    "\n",
    "$\\hat{d} = 0.19832 - 0.21887 = -0.0255$\n",
    "\n",
    "Confidence Interval = point estimate +/- z-score * standard error\n",
    "\n",
    "Confidence Interval = $\\hat{d}$ +/- z-score-for-95%-interval * standard error-pooled\n",
    "\n",
    "Confidence Interval = -0.02055 +/- 1.96 * 0.004372\n",
    "\n",
    "Confidence Interval = -0.02055 +/- 0.008569\n",
    "\n",
    "So the confidence interval is (-0.02912 to -0.011981), rounded to (-0.0291 to -0.0120).\n",
    "\n",
    "The confidence interval does NOT include 0, so the result is statistically significant. The confidence interval also does NOT include -0.01, so the result is practically significant also. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Net Conversion**\n",
    "\n",
    "Control Group:\n",
    "Clicks = 17,293. Enrollments = 3785. Payments = 2033\n",
    "\n",
    "Test Group:\n",
    "Clicks = 17,260. Enrollments = 3423. Payments = 1945\n",
    "\n",
    "$\\hat{P}pooled = \\frac{Xcontrol + Xtest}{Ncontrol+Ntest}$\n",
    "\n",
    "$SD-pooled = \\sqrt{\\hat{P}pool(1-\\hat{P}pool)(\\frac{1}{Ncontrol}+\\frac{1}{Ntest})}$\n",
    "\n",
    "$\\hat{P}pooled\\_to\\_pay = \\frac{2033+1945}{17,293+17,260} = 0.11513$\n",
    "\n",
    "$SD-pooled\\_to\\_pay = \\sqrt{0.1151(1-0.1151)(\\frac{1}{17,293}+\\frac{1}{17,260})} = 0.003434$\n",
    "\n",
    "$\\hat{P}control = \\frac{2033}{17,293} = 0.121756$\n",
    "\n",
    "$\\hat{P}test = \\frac{1945}{17,260} = 0.11269$\n",
    "\n",
    "$H_0: P(test\\_group) - P(control\\_group) = 0; d = 0$\n",
    "\n",
    "$H_A: P(test\\_group) - P(control\\_group) = 0; d\\neq 0$\n",
    "\n",
    "$\\hat{d} = 0.11269 - 0.121756 = -0.004874 = -0.49\\%$\n",
    "\n",
    "Confidence Interval = point estimate +/- z-score * standard error\n",
    "\n",
    "Confidence Interval = $\\hat{d}$ +/- z-score-for-95%-interval * standard error-pooled\n",
    "\n",
    "Confidence Interval = -0.00487 +/- 1.96 * 0.00343\n",
    "\n",
    "Confidence Interval = -0.00487 +/- 0.006723\n",
    "\n",
    "So the confidence interval is (-0.01160 to 0.00186), rounded to (-0.0116 to 0.0019).\n",
    "\n",
    "The confidence interval DOES include 0, so the result is not statistically significant. The confidence interval also DOES include -0.0075 (d-min), so the result is NOT practically significant either.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Bonferroni Correction**\n",
    "\n",
    "We measured two metrics in one experiment. Applying the Bonferroni correction means that the alpha-level for each hypothesis would be 2.5% instead of 5%. The result is the confidence intervals would also be significantly wider. This is likely too conservative.\n",
    "For our experiment, it is likely that the gross conversion and net conversion are related, not independent. So if one metric changes, the other is also likely to change. So using the conservative Bonferroni correction might cause us to fail to detect a signficant change that exists.\n",
    "\n",
    "If we were testing several metrics (not just two), the Bonferroni correction would be more important to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oy_RAh_uyThF"
   },
   "source": [
    "## E3. Run Sign Tests\n",
    "For each evaluation metric, do a Sign test using the day-by-day breakdown. If the sign test does not agree with the confidence interval for the difference, see if you can figure out why.\n",
    "\n",
    "---\n",
    "We can also use a Sign test to see what is the chance that the test group randomly has higher click-through than the control group each day. One good attribute of the Sign test is that it does not assume a population has a particular distribution, such as a normal distribution. This allows the Sign test to be used when a popluation's distribution is not known.\n",
    "\n",
    "Here we calculate the value of the metric (ie, gross conversion or net conversion) each day. Then we count how many days the metric was higher in the test group than the control group. This will be the number of successes for our binomial variable. We assume the chance the test group is higher is random (ie, 50% chance). Then we use the binomial distribution with **p = 0.5**  and **n = the number of experiments (days)** to tell us the probability that **x days** will be a success (ie, the test group is higher than the control group) over those n days.\n",
    "\n",
    "I used an online calculator (Reference #6) to calculate this quickly.\n",
    "\n",
    "From the data given:\n",
    "\n",
    "Pageviews = 23 days\n",
    "\n",
    "Clicks = 23 days\n",
    "\n",
    "Enrollments = 23 days\n",
    "\n",
    "Payments = 23 days\n",
    "\n",
    "For gross conversion, the test group had a higher percentage than the control group on 4 days. (\"Successes\" = 4)\n",
    "\n",
    "For net conversion, the test group had a higher percentage than the control group on 10 days. (\"Successes\" = 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bn5hej44sRdC"
   },
   "source": [
    "Here is the calculation for gross conversion.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1DPSdfyuzgjF3DMW9FG5DxkqbQ4S5Rbia)\n",
    "\n",
    "The chance of seeing 4 out of 23 times that the test group has a higher click through (if this probability is 50% for each day) is 0.26%. This is less than our level of significance, 5%. So this is very unlikely and therefore not due to chance. So it means that the test group IS different than the control group. This agrees with the result we got from the analytical test for the gross conversion rate.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFsx44m7tnhi"
   },
   "source": [
    "Here is the calculation for net conversion.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1rgkZj0RrXYgcSC4ESeSP4RzYa7juSUdn)\n",
    "\n",
    "The chance of seeing 10 out of 23 times that the test group has a higher click through (if this probability is 50%) is 68%. This is greater than our level of significance, 5%. So this is very likely and therefore is due to chance. So it means that the test group IS NOT different than the control group. This agrees with the result we got from the analytical test for the net conversion rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Le_mJ9hryMnA"
   },
   "source": [
    "## E4. Make a Recommendation\n",
    "Finally, make a recommendation. Would you launch this experiment, not launch it, dig deeper, run a follow-up experiment, or is it a judgment call? If you would dig deeper, explain what area you would investigate. If you would run follow-up experiments, briefIy describe that experiment. If it is a judgment call, explain what factors would be relevant to the decision.\n",
    "\n",
    "\n",
    "---\n",
    "**Gross conversion**\n",
    "\n",
    "Based on the initial exploration, we expect the free trial screener will reduce the number of people who start the free trial who are not able to commit enough time to the course. The gross conversion in the test group is around 2.06% smaller than the gross conversion in the control group. This difference is both statistically and practically significant, as the minimum practical signifcance was 1%. Also, the range of values in the 95% confidence interval are all negative, which means there is a high level of confidence that the screener reduces the gross conversion rate.  This will reduce the number of students who are not able to commit the time needed to pass the course. It should lower the number of students not finishing the course and will allow mentoring resources to be better used on students who are seriously committed to the course.\n",
    "\n",
    "**Net conversion**\n",
    "\n",
    "The net conversion rate for the test group is about 0.49% smaller than the control group. Although this is not statistically significantly different from the control group using a 95% confidence interval, it is still a decrease in net conversion. In addition, the 95% confidence interval ranges from -1.16% to +0.19%, so most of that range includes a decrease in net conversion. (Usually changes of 1% or more in conversion are considered significant.) Given that Udacity's goal is for gross conversion to decrease but net conversion to remain the same or possibly increase, I would NOT LAUNCH the experiment as the net conversion may decrease by up to 1.16%. \n",
    "\n",
    "If more time is available, I would run the test again and see if the net conversion confidence interval no longer includes a decrease in the net conversion rate, or contains a much small decrease in conversion rate than -1.16%. If the second experiment shows a much lower decrease in net conversion rate, then it might be recommended to run the full-scale experiment. Otherwise, I would recommend spending time and resources on other experiments which may prove more productive.\n",
    "\n",
    "**Overall Conclusion:**  Do NOT launch the experiment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Google-Udacity A/B Testing-Final Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
